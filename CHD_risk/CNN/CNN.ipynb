{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Classification of CHD cases\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q ydata-profiling numpy pandas matplotlib tensorflow imblearn torch \n",
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display, Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "def to_list(df_dict, heads):\n",
    "    \"\"\"Convert dataframe to dictionaries\"\"\"\n",
    "    # {key: [], ...}\n",
    "    out = {}\n",
    "    for _key in list(heads):\n",
    "        out[_key] = [df_dict[_key][key] for key in df_dict[_key].keys()]\n",
    "    return out\n",
    "\n",
    "def encode_input(data_range, data):\n",
    "    \"\"\"Encode categorical variables with string categories into numerical categoriess\"\"\"\n",
    "    return data_range.index(data)\n",
    "\n",
    "def preprocess_data(df_list: dict, normalize=True) -> np.ndarray:\n",
    "    \"\"\"Preprocess data into np.arrays (encode if needed)\"\"\"\n",
    "    output = []\n",
    "    data_length = len(df_list['chd'])\n",
    "    for i in range(data_length):\n",
    "        item = []\n",
    "        for key in list(df_list.keys())[1:]:\n",
    "            item.append(df_list[key][i])\n",
    "        output.append(item)\n",
    "    inputs_arr = np.array(output)\n",
    "    targets_arr = np.array(df_list['chd'])\n",
    "    if normalize:\n",
    "        _range = np.max(inputs_arr, axis=0) - np.min(inputs_arr, axis=0)\n",
    "        inputs_arr = (inputs_arr-np.min(inputs_arr, axis=0)) / _range\n",
    "    return inputs_arr, targets_arr\n",
    "\n",
    "def prepare_data(inputs, targets, seed=1001):\n",
    "    \"\"\"Prepare data for CNN\"\"\"\n",
    "    positive_mask = targets == 1\n",
    "    negative_mask = targets == 0\n",
    "    \n",
    "    #Choose between a. and b. \n",
    "        # a. Use the next 6 lines of code if need to balance sample by undersampling\n",
    "    np.random.seed(seed)\n",
    "    n_minimum = min(np.sum(positive_mask), np.sum(negative_mask))\n",
    "\n",
    "    positive_indices = random.sample(range(np.sum(positive_mask)), n_minimum)\n",
    "    negative_indices = random.sample(range(np.sum(negative_mask)), n_minimum)\n",
    "\n",
    "    positive_inputs = inputs[positive_mask][positive_indices, ]\n",
    "    positive_targets = targets[positive_mask][positive_indices, ]\n",
    "    negative_inputs = inputs[negative_mask][negative_indices,]\n",
    "    negative_targets = targets[negative_mask][negative_indices,]\n",
    "    inputs = np.concatenate([positive_inputs, negative_inputs]).tolist()\n",
    "    targets = np.concatenate([positive_targets, negative_targets]).tolist()\n",
    "\n",
    "        # b. Use next two lines if no need to balance samples \n",
    "    #inputs = inputs.tolist()\n",
    "    #targets = targets.tolist()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(inputs)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(targets)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "def get_metrics(y_pred, y_true):\n",
    "    \"\"\"Calculate metrics from confusion matrix\"\"\"\n",
    "    _confusion_matrix = confusion_matrix(y_pred, y_true)\n",
    "    tp = _confusion_matrix[0,0]\n",
    "    fn = _confusion_matrix[1,0]\n",
    "    fp = _confusion_matrix[0,1]\n",
    "    tn = _confusion_matrix[1,1]\n",
    "    # metrics\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    fscore = 2*tp/(2*tp + fp + fn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    miss_rate = fn/(tn+tp)\n",
    "    fall_out_rate = fp/(fp+tn)\n",
    "    # return [precision, recall, fscore, accuracy, miss_rate, fall_out_rate]\n",
    "    return [precision, recall, fscore, accuracy, miss_rate, fall_out_rate]\n",
    "\n",
    "def present_metrics(results_dict): \n",
    "    \"\"\"Generate data frame of metrics\"\"\"\n",
    "    df = pd.DataFrame(results_dict, index = ['Precision', 'Recall', 'F-score', 'Accuracy', 'Miss Rate', 'Fall out rate'])\n",
    "    df['Average'] = df.mean(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    \"\"\"Plot train and test loss (left) / accuracy (right) for each epoch\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    colors = plt.cm.get_cmap('tab10', len(history))  \n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    for i, (repeat, data) in enumerate(history.items()):\n",
    "        epochs = range(1, len(data['train_loss']) + 1)\n",
    "        ax1.plot(epochs, data['train_loss'], '--', label=f'{repeat} Training Loss',color=colors(i))\n",
    "        ax1.plot(epochs, data['val_loss'], label=f'{repeat} Validation Loss',color=colors(i))\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    for i, (repeat, data) in enumerate(history.items()):\n",
    "        epochs = range(1, len(data['train_acc']) + 1)\n",
    "        ax2.plot(epochs, data['train_acc'], '--', label=f'{repeat} Training Accuracy',color=colors(i))\n",
    "        ax2.plot(epochs, data['val_acc'], label=f'{repeat} Validation Accuracy',color=colors(i))\n",
    "\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imbalanced data and convert to dictionary \n",
    "filepath = 'cleaned_data_imbalanced.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df_clear = df.dropna(axis=0)\n",
    "df_shuffle = df_clear.sample(frac=1).reset_index(drop=True)\n",
    "df_dict = df_shuffle.to_dict() # shuffle resampled data\n",
    "heads = list(df_dict.keys())\n",
    "df_list = to_list(df_dict, heads)\n",
    "display('Number of samples: {len(df_list[\"chd\"])}')\n",
    "display('Number of variables: {len(df_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN\n",
    "\n",
    "class CHDPred(Dataset):\n",
    "    def __init__(self, inputs, targets) -> None:\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _input = torch.from_numpy(self.inputs[index]).type(torch.float32)\n",
    "        _target = torch.from_numpy(self.targets[index]).type(torch.float32)\n",
    "        return _input, _target\n",
    "\n",
    "# Actual classification model \n",
    "class CHDPredModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CHDPredModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,2)) # kernel size adjusted to fit dataset\n",
    "\n",
    "        self.linear1 = nn.Linear(32, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view((-1, 32))\n",
    "        out = F.relu(self.linear1(out))\n",
    "        return F.sigmoid(self.linear2(out))\n",
    "\n",
    "# Classification model for shape check in each step\n",
    "class CHDPredModelSHAPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CHDPredModelSHAPE, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,2)) # kernel size adjusted to fit dataset\n",
    "        self.linear1 = nn.Linear(32, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        print(\"Shape after conv1:\", out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        print(\"Shape after conv2:\", out.shape)\n",
    "        out = out.view((-1, 32))\n",
    "        print(\"Shape after view:\", out.shape)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        print(\"Shape after linear1:\", out.shape)\n",
    "        return torch.sigmoid(self.linear2(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = preprocess_data(df_list)\n",
    "print(f'inputs shape after preprocess_data(): {inputs.shape}')\n",
    "print(f'targets shape after preprocess_data(): {targets.shape}')\n",
    "\n",
    "inputs, targets = prepare_data(inputs, targets)\n",
    "print(f'inputs shape after prepare_data(): {inputs.shape}')\n",
    "print(f'targets shape after prepare_data(): {targets.shape}')\n",
    "\n",
    "n_samples = inputs.shape[0]\n",
    "tr_inputs = inputs[0:int(n_samples*0.7), :].reshape((-1, 1, 3, 5)) # input size adjusted to fit dataset\n",
    "print(f'tr_inputs shape: {tr_inputs.shape}')\n",
    "tr_targets = targets[0:int(n_samples*0.7)].reshape((-1, 1))\n",
    "print(f'tr_targets shape: {tr_targets.shape}')\n",
    "te_inputs = inputs[int(n_samples*0.7):, :].reshape((-1, 1, 3, 5)) # input size adjusted to fit dataset\n",
    "print(f'te_inputs shape: {te_inputs.shape}')\n",
    "te_targets = targets[int(n_samples*0.7):].reshape((-1, 1))\n",
    "print(f'te_targets shape: {te_targets.shape}')\n",
    "\n",
    "# Check shapes of the model \n",
    "batchsize = 32\n",
    "\n",
    "train_set = CHDPred(tr_inputs, tr_targets)\n",
    "val_set = CHDPred(te_inputs, te_targets)\n",
    "train_loader = DataLoader(train_set, batch_size=batchsize)\n",
    "val_loader = DataLoader(val_set, batch_size=1)\n",
    "\n",
    "model = CHDPredModelSHAPE()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for j, (input_, target_) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(input_)\n",
    "    loss = criterion(out, target_)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Printing shapes for the first iteration only\n",
    "    if j == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "repeats = 5\n",
    "batchsize = 32\n",
    "\n",
    "all_results = {}\n",
    "history = {}\n",
    "\n",
    "for rep in range(repeats): \n",
    "    \n",
    "    display(Markdown(f'#### Repeat {rep+1}'))\n",
    "\n",
    "    #inputs, targets = preprocess_data(df_list)\n",
    "    #inputs, targets = prepare_data(inputs, targets)\n",
    "    #n_samples = inputs.shape[0]\n",
    "    #tr_inputs = inputs[0:int(n_samples*0.7), :].reshape((-1, 1, 3, 5)) # input size adjusted to fit dataset\n",
    "    #tr_targets = targets[0:int(n_samples*0.7)].reshape((-1, 1))\n",
    "    #te_inputs = inputs[int(n_samples*0.7):, :].reshape((-1, 1, 3, 5)) # input size adjusted to fit dataset\n",
    "    #te_targets = targets[int(n_samples*0.7):].reshape((-1, 1))\n",
    "\n",
    "    train_set = CHDPred(tr_inputs, tr_targets)\n",
    "    val_set = CHDPred(te_inputs, te_targets)\n",
    "    train_loader = DataLoader(train_set, batch_size=batchsize)\n",
    "    val_loader = DataLoader(val_set, batch_size=1)\n",
    "\n",
    "    model = CHDPredModel()\n",
    "    citeration = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Initiation accuracy and loss records \n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        loss_ = 0\n",
    "        acc_ = 0\n",
    "        val_acc = 0\n",
    "        val_loss_ = 0\n",
    "\n",
    "        for j, (input_, target_) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_)\n",
    "            loss = citeration(out, target_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = out.detach().numpy()\n",
    "            pred_ = np.zeros_like(pred)\n",
    "            pred_[pred>0.5] = 1\n",
    "            pred_ = pred_.astype('float')\n",
    "            acc = np.sum(pred_ == target_.numpy()) / batchsize\n",
    "            \n",
    "            loss_ += loss.item()\n",
    "            acc_ += acc\n",
    "\n",
    "        model.eval()\n",
    "        for j, (input_, target_) in enumerate(val_loader):\n",
    "            out = model(input_)\n",
    "            val_loss = citeration(out, target_)\n",
    "\n",
    "            pred = out.detach().numpy()\n",
    "            pred_ = np.zeros_like(pred)\n",
    "            pred_[pred>0.5] = 1\n",
    "            pred_ = pred_.astype('float')\n",
    "            acc = np.sum(pred_ == target_.numpy())\n",
    "            \n",
    "            val_loss_ += val_loss.item()\n",
    "            val_acc += acc\n",
    "\n",
    "        # Calculate average training and validation accuracy and loss\n",
    "        train_acc_avg = acc_ / len(train_loader)\n",
    "        val_acc_avg = val_acc / len(val_loader)\n",
    "        train_loss_avg = loss_ / len(train_loader)\n",
    "        val_loss_avg = val_loss_ / len(val_loader)\n",
    "\n",
    "        # Store values for plotting\n",
    "        train_accs.append(train_acc_avg)\n",
    "        val_accs.append(val_acc_avg)\n",
    "        train_losses.append(train_loss_avg)\n",
    "        val_losses.append(val_loss_avg)\n",
    "\n",
    "        # Display after each epoch\n",
    "        print(\"epochs: {}, train_loss: {}, val_loss: {}, train_acc: {}, val_acc: {}\".format(\n",
    "        i + 1,\n",
    "        train_loss_avg,\n",
    "        val_loss_avg,\n",
    "        train_acc_avg, \n",
    "        val_acc_avg))\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for j, (input_, target_) in enumerate(val_loader):\n",
    "        out = model(input_)\n",
    "        pred = out.detach().numpy()\n",
    "        pred_ = np.zeros_like(pred)\n",
    "        pred_[pred>0.5] = 1\n",
    "        pred_ = pred_.astype('float')\n",
    "        preds.append(pred_[0][0])\n",
    "        labels.append(target_.numpy()[0][0])\n",
    "\n",
    "    # Record history in dictionary\n",
    "    repeat_key = f'Repeat {len(all_results) + 1}'\n",
    "        # history = {'Repeat 1': [loss, val_acc]} \n",
    "    history[repeat_key] = {'train_acc': train_accs,\n",
    "                           'val_acc': val_accs, \n",
    "                           'train_loss': train_losses,\n",
    "                           'val_loss': val_losses}\n",
    "    \n",
    "    # Record results in dictionary\n",
    "    round_results = get_metrics(preds, labels)\n",
    "    all_results[repeat_key] = round_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "plot_loss_acc(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance metrics\n",
    "display(Markdown(f'# CNN Performance Metrics'))\n",
    "display(present_metrics(all_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('model_scripted.pt') # Save"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f07d5c506dc792c1d17042ce6d63d3539913070c7203ee1d707a2b2ce1ee992d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
